# Ref: https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/time-series/Simple_RNN.ipynb

""""
This is a regression problem: can we train an RNN to accurately predict the next data point, given a current data point?
"""


import torch
from torch import nn
import numpy as np
import matplotlib.pyplot as plt

from common.save_fig import save_fig


class RNN(nn.Module):
    def __init__(self, input_size, output_size, hidden_dim, n_layers):
        super(RNN, self).__init__()

        self.hidden_dim = hidden_dim

        # define an RNN with specified parameters
        # batch_first means that the first dim of the input and output will be the batch_size
        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)

        # last, fully-connected layer
        self.fc = nn.Linear(hidden_dim, output_size)

    def forward(self, x, hidden):
        # x (batch_size, seq_length, input_size)
        # hidden (n_layers, batch_size, hidden_dim)
        # r_out (batch_size, time_step, hidden_size)
        batch_size = x.size(0)

        # get RNN outputs
        r_out, hidden = self.rnn(x, hidden)
        # shape output to be (batch_size*seq_length, hidden_dim)
        r_out = r_out.view(-1, self.hidden_dim)

        # get final output
        output = self.fc(r_out)

        return output, hidden


# decide on hyperparameters
input_size = 1
output_size = 1
hidden_dim = 32
n_layers = 1

# instantiate an RNN
rnn = RNN(input_size, output_size, hidden_dim, n_layers)
print(rnn)

"""
RNN(
  (rnn): RNN(1, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)
"""


criterion = nn.MSELoss()
optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)

"""
Hidden State
Pay close attention to the hidden state, here:

Before looping over a batch of training data, the hidden state is initialized
After a new hidden state is generated by the rnn, we get the latest hidden state, and use that as input to the rnn for 
the following steps
"""


def train(rnn, n_steps, print_every, x_tensor, y_tensor, time_steps):

    # initialize hidden state
    hidden = None

    for batch_i, step in enumerate(range(n_steps)):

        # outputs from the rnn
        prediction, hidden = rnn(x_tensor, hidden)

        ## Representing Memory ##
        # make a new variable for hidden and detach the hidden state from its history
        # this way, we don't backpropagate through the entire history
        hidden = hidden.data

        # calculate the loss
        loss = criterion(prediction, y_tensor)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # display loss and predictions
        if batch_i % print_every == 0:
            print('Loss: ', loss.item())
            plt.figure(figsize=(8, 5))
            plt.plot(time_steps[1:], x, 'r.', label='input')  # input
            plt.plot(time_steps[1:], prediction.data.numpy().flatten(), 'b.', label='prediction')  # predictions
            plt.legend()
            fig = plt.gcf()
            name = 'time_series_pred' + str(batch_i)
            save_fig('RNN/images', fig, name)

    return rnn


# define training data
# how many time steps/data pts are in one batch of data
seq_length = 20
step = 0
time_steps = np.linspace(step * np.pi, (step + 1) * np.pi, seq_length + 1)

data = np.sin(time_steps)
data.resize((seq_length + 1, 1))  # input_size=1

x = data[:-1]
y = data[1:]

# convert data into Tensors
x_tensor = torch.Tensor(x).unsqueeze(0)  # unsqueeze gives a 1, batch_size dimension
y_tensor = torch.Tensor(y)

# train the rnn and monitor results
n_steps = 75
print_every = 15

trained_rnn = train(rnn, n_steps, print_every, x_tensor, y_tensor, time_steps)






